{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /usr/local/lib/python2.7/dist-packages/jieba/dict.txt ...\n",
      "DEBUG:jieba:Building prefix dict from /usr/local/lib/python2.7/dist-packages/jieba/dict.txt ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Loading model from cache /tmp/jieba.cache\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Dumping model to file cache /tmp/jieba.cache\n",
      "Dump cache file failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/jieba/__init__.py\", line 103, in initialize\n",
      "    replace_file(fpath, cache_file)\n",
      "OSError: [Errno 1] Operation not permitted\n",
      "ERROR:jieba:Dump cache file failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/jieba/__init__.py\", line 103, in initialize\n",
      "    replace_file(fpath, cache_file)\n",
      "OSError: [Errno 1] Operation not permitted\n",
      "Loading model cost 1.20809912682 seconds.\n",
      "DEBUG:jieba:Loading model cost 1.20809912682 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 :  語音 我們 文字 服務 django 位置 方式 套件 解決\n",
      "Topic 1 :  python 如何 使用 一個 問題 如果 設計 經驗 eda\n",
      "Topic 2 :  applications create static map api use time google interface\n",
      "Topic 3 :  data network information mobile fields analytics protocol practice learn\n",
      "Topic 4 :  介紹 hdf5 ceph 利用 測試 ipython 存取 notebook pytest\n",
      "Topic 5 :  python sphinx programming debugger use language explain documentation process\n",
      "Topic 6 :  swift developers nodes benchmark cloud building service ansible cluster\n",
      "Topic 7 :  technology lcd program happy designers programmers order provides tasks\n",
      "Topic 8 :  python talk like analysis used learning use tools ll\n",
      "Topic 9 :  using make gandi manage development pycon services physics session\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "import csv\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer , TfidfTransformer\n",
    "from sklearn.decomposition import NMF\n",
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "\n",
    "from lxml.html.soupparser import fromstring\n",
    "from bs4 import UnicodeDammit\n",
    "\n",
    "import lxml.etree as ET\n",
    "\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "\n",
    "\n",
    "jieba.load_userdict(\"new.dict_all\")\n",
    "# import jieba.analyse\n",
    "# jieba.analyse.load_stop_words(\"stop_words_list.txt\")\n",
    "\n",
    "\n",
    "def uri_to_file_name(uri):\n",
    "    return uri.replace(\"/\", \"-\")\n",
    "\n",
    "sessions = {}\n",
    "\n",
    "xpath_abstract = '''//div[@class='panel-body']/div[1]/text()'''\n",
    "\n",
    "with open(\"data/sc.csv\", \"r\") as sessions_file:\n",
    "    for row in csv.DictReader(sessions_file, ['title', 'link', 'speaker']):  \n",
    "        session_id = (row['title'])\n",
    "        filename = \"data/sessions/\" + uri_to_file_name(row['link']) + '.html'\n",
    "        page = open(filename).read()\n",
    "        soup = fromstring(page)\n",
    "        ab = soup.xpath(xpath_abstract)\n",
    "        if len(ab) >0 :\n",
    "            abstract = ab[0].replace('\\n', ' ').replace('\\r', '').replace('\\r\\n', '').encode('utf-8','ignore')\n",
    "            title = row['title']\n",
    "            sessions[row['link']] = {'title':title , 'abstract':abstract}\n",
    "           \n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "        \n",
    "corpus = []\n",
    "titles = []\n",
    "for id, session in sorted(sessions.iteritems(), key=lambda t: t[0]):\n",
    "    wordlist=pseg.cut(session[\"abstract\"] + session[\"title\"])\n",
    "    words = ''\n",
    "    for key in wordlist:  \n",
    "         words  = words + ' ' + key.word\n",
    "    corpus.append(words)\n",
    "    titles.append(session[\"title\"])\n",
    "\n",
    "n_topics = 10\n",
    "n_top_words = 10\n",
    "n_features = 6000\n",
    "\n",
    "# vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,1), min_df = 0, stop_words = 'english')\n",
    "# vectorizer = CountVectorizer(analyzer='word', ngram_range=(1,1), min_df = 0, stop_words = 'english')\n",
    "\n",
    "\n",
    "if False :    \n",
    "    vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,1), min_df = 0, stop_words = 'english')\n",
    "    tfidf =  vectorizer.fit_transform(corpus)\n",
    "    word = vectorizer.get_feature_names()\n",
    "    \n",
    "else :\n",
    "    vectorizer = CountVectorizer(analyzer='word', ngram_range=(1,1), min_df = 0, stop_words = 'english')\n",
    "    tfidf = vectorizer.fit_transform(corpus)\n",
    "    word=vectorizer.get_feature_names()\n",
    "\n",
    "\n",
    "\n",
    "import lda\n",
    "import numpy as np\n",
    "# \n",
    "vocab = word\n",
    "\n",
    "\n",
    "\n",
    "if True:\n",
    "    model = lda.LDA(n_topics=n_topics, n_iter=500, random_state=1)\n",
    "    model.fit(tfidf)\n",
    "    topic_word = model.topic_word_\n",
    "    for i, topic_dist in enumerate(topic_word):\n",
    "        topic_words = np.array(vocab)[np.argsort(topic_dist)][:-n_top_words:-1]\n",
    "        s = ''\n",
    "        for word in topic_words :\n",
    "            s = s + ' ' + word.encode('utf-8')\n",
    "        print('Topic ' + str(i) + ' : ' + s )\n",
    "else :\n",
    "    # Fit the NMF model\n",
    "    print(\"Fitting the NMF model with n_samples=%d and n_features=%d...\"% (tfidf.shape[0], n_features))\n",
    "    nmf = NMF(n_components=n_topics, random_state=1).fit(tfidf)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    for topic_idx, topic in enumerate(nmf.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        print()\n",
    "\n",
    "\n",
    "# \n",
    "# doc_topic = model.doc_topic_\n",
    "# for i in range(0, len(titles)):\n",
    "#     print(\"{} (top topic: {})\".format(titles[i], doc_topic[i].argmax()))\n",
    "#     print(doc_topic[i].argsort()[::-1][:3])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
