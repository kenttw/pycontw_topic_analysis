{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /Library/Python/2.7/site-packages/jieba/dict.txt ...\n",
      "DEBUG:jieba:Building prefix dict from /Library/Python/2.7/site-packages/jieba/dict.txt ...\n",
      "Loading model from cache /var/folders/9k/pdcz30p16nz8_ngb0l5h_5kw0000gp/T/jieba.cache\n",
      "DEBUG:jieba:Loading model from cache /var/folders/9k/pdcz30p16nz8_ngb0l5h_5kw0000gp/T/jieba.cache\n",
      "Loading model cost 0.835110187531 seconds.\n",
      "DEBUG:jieba:Loading model cost 0.835110187531 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 :  python 如何 使用 問題 一個 hdf5 設計 套件 撰寫\n",
      "Topic 1 :  語音 我們 文字 位置 測試 每個 內容 影像 pytest\n",
      "Topic 2 :  python use applications create ll using make easy static\n",
      "Topic 3 :  python like used analysis learning debugger sentiment various interface\n",
      "Topic 4 :  swift sphinx nodes benchmark cloud gandi manage services process\n",
      "Topic 5 :  python 介紹 ceph 服務 利用 如果 特性 互動 呈現\n",
      "Topic 6 :  django technology program lcd happy programmers designers work assembly\n",
      "Topic 7 :  data network information mobile fields analytics modern analysis detecting\n",
      "Topic 8 :  python talk developers building build tools written scale experience\n",
      "Topic 9 :  talk python programming pycon important development future debugging gui\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "import csv\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer , TfidfTransformer\n",
    "from sklearn.decomposition import NMF\n",
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "\n",
    "from lxml.html.soupparser import fromstring\n",
    "from bs4 import UnicodeDammit\n",
    "\n",
    "import lxml.etree as ET\n",
    "\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "\n",
    "\n",
    "jieba.load_userdict(\"new.dict_all\")\n",
    "# import jieba.analyse\n",
    "# jieba.analyse.load_stop_words(\"stop_words_list.txt\")\n",
    "\n",
    "\n",
    "def uri_to_file_name(uri):\n",
    "    return uri.replace(\"/\", \"-\")\n",
    "\n",
    "sessions = {}\n",
    "\n",
    "xpath_abstract = '''//div[@class='panel-body']/div[1]/text()'''\n",
    "\n",
    "with open(\"data/sc.csv\", \"r\") as sessions_file:\n",
    "    for row in csv.DictReader(sessions_file, ['title', 'link', 'speaker']):  \n",
    "        session_id = (row['title'])\n",
    "        filename = \"data/sessions/\" + uri_to_file_name(row['link']) + '.html'\n",
    "        page = open(filename).read()\n",
    "        soup = fromstring(page)\n",
    "        ab = soup.xpath(xpath_abstract)\n",
    "        if len(ab) >0 :\n",
    "            abstract = ab[0].replace('\\n', ' ').replace('\\r', '').replace('\\r\\n', '').encode('utf-8','ignore')\n",
    "            title = row['title']\n",
    "            sessions[row['link']] = {'title':title , 'abstract':abstract}\n",
    "           \n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "        \n",
    "corpus = []\n",
    "titles = []\n",
    "for id, session in sorted(sessions.iteritems(), key=lambda t: t[0]):\n",
    "    wordlist=pseg.cut(session[\"abstract\"] + session[\"title\"])\n",
    "    words = ''\n",
    "    for key in wordlist:  \n",
    "         words  = words + ' ' + key.word\n",
    "    corpus.append(words)\n",
    "    titles.append(session[\"title\"])\n",
    "\n",
    "n_topics = 10\n",
    "n_top_words = 10\n",
    "n_features = 6000\n",
    "\n",
    "# vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,1), min_df = 0, stop_words = 'english')\n",
    "# vectorizer = CountVectorizer(analyzer='word', ngram_range=(1,1), min_df = 0, stop_words = 'english')\n",
    "\n",
    "\n",
    "if False :    \n",
    "    vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,1), min_df = 0, stop_words = 'english')\n",
    "    tfidf =  vectorizer.fit_transform(corpus)\n",
    "    word = vectorizer.get_feature_names()\n",
    "    \n",
    "else :\n",
    "    vectorizer = CountVectorizer(analyzer='word', ngram_range=(1,1), min_df = 0, stop_words = 'english')\n",
    "    tfidf = vectorizer.fit_transform(corpus)\n",
    "    word=vectorizer.get_feature_names()\n",
    "\n",
    "\n",
    "\n",
    "import lda\n",
    "import numpy as np\n",
    "# \n",
    "vocab = word\n",
    "\n",
    "\n",
    "\n",
    "if True:\n",
    "    model = lda.LDA(n_topics=n_topics, n_iter=500, random_state=1)\n",
    "    model.fit(tfidf)\n",
    "    topic_word = model.topic_word_\n",
    "    for i, topic_dist in enumerate(topic_word):\n",
    "        topic_words = np.array(vocab)[np.argsort(topic_dist)][:-n_top_words:-1]\n",
    "        s = ''\n",
    "        for word in topic_words :\n",
    "            s = s + ' ' + word.encode('utf-8')\n",
    "        print('Topic ' + str(i) + ' : ' + s )\n",
    "else :\n",
    "    # Fit the NMF model\n",
    "    print(\"Fitting the NMF model with n_samples=%d and n_features=%d...\"% (tfidf.shape[0], n_features))\n",
    "    nmf = NMF(n_components=n_topics, random_state=1).fit(tfidf)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    for topic_idx, topic in enumerate(nmf.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        print()\n",
    "\n",
    "\n",
    "# \n",
    "# doc_topic = model.doc_topic_\n",
    "# for i in range(0, len(titles)):\n",
    "#     print(\"{} (top topic: {})\".format(titles[i], doc_topic[i].argmax()))\n",
    "#     print(doc_topic[i].argsort()[::-1][:3])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
